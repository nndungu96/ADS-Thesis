{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c921c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold, learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0675f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data and remove redundant columns\n",
    "df_metrics = pd.read_csv('Data/metrics_corr.csv')\n",
    "kl_list = [col for col in df_metrics.columns if col.endswith('_kl')]\n",
    "df_metrics.drop(kl_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4be7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify age\n",
    "df_metrics.loc[(df_metrics['DoB']>1971) & (df_metrics['DoB']<=1983), 'age_class'] = 0\n",
    "df_metrics.loc[(df_metrics['DoB']>1983) & (df_metrics['DoB']<=1994), 'age_class'] = 1\n",
    "df_metrics.loc[(df_metrics['DoB']>1994) & (df_metrics['DoB']<=2000), 'age_class'] = 2\n",
    "df_metrics.loc[(df_metrics['DoB']>2000) & (df_metrics['DoB']<=2011), 'age_class'] = 3\n",
    "\n",
    "df_metrics['age_class'] = pd.to_numeric(df_metrics['age_class'], downcast = 'integer')\n",
    "df_metrics = df_metrics[df_metrics.age_class.isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d925d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorize gender\n",
    "le = LabelEncoder()\n",
    "df_metrics['age_var'] = le.fit_transform(df_metrics['age_class'])\n",
    "df_metrics['gender_var'] = le.fit_transform(df_metrics['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "749ddf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists that store column names for each saliency map\n",
    "\n",
    "aim_list = [col for col in df_metrics.columns if col.startswith('aim_')]\n",
    "rare_list = [col for col in df_metrics.columns if col.startswith('rare_')]\n",
    "qss_list = [col for col in df_metrics.columns if col.startswith('qss_')]\n",
    "lds_list = [col for col in df_metrics.columns if col.startswith('lds_')]\n",
    "imsig_list = [col for col in df_metrics.columns if col.startswith('imsig_')]\n",
    "ikn_list = [col for col in df_metrics.columns if col.startswith('ikn_')]\n",
    "gbvs_list = [col for col in df_metrics.columns if col.startswith('gbvs_')]\n",
    "gaus_list = [col for col in df_metrics.columns if col.startswith('gaus_')]\n",
    "fes_list = [col for col in df_metrics.columns if col.startswith('fes_')]\n",
    "dva_list = [col for col in df_metrics.columns if col.startswith('dva_')]\n",
    "cvs_list = [col for col in df_metrics.columns if col.startswith('cvs_')]\n",
    "cas_list = [col for col in df_metrics.columns if col.startswith('cas_')]\n",
    "dgi_list = [col for col in df_metrics.columns if col.startswith('dgi_')]\n",
    "dgii_list = [col for col in df_metrics.columns if col.startswith('dgii_')]\n",
    "dgiie_list = [col for col in df_metrics.columns if col.startswith('dgiie_')]\n",
    "icf_list = [col for col in df_metrics.columns if col.startswith('icf_')]\n",
    "all_pred = icf_list + dgiie_list + dgii_list + dgi_list + cas_list + cvs_list + dva_list + fes_list + gbvs_list + ikn_list + imsig_list + lds_list + qss_list + rare_list + aim_list\n",
    "d_aim_list = [col for col in df_metrics.columns if col.startswith('d_aim_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c52b43c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3afca0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>fixations</th>\n",
       "      <th>fixmap</th>\n",
       "      <th>aim_nss</th>\n",
       "      <th>aim_auc</th>\n",
       "      <th>aim_ig</th>\n",
       "      <th>aim_cc</th>\n",
       "      <th>aim_sauc</th>\n",
       "      <th>aim_sim</th>\n",
       "      <th>rare_nss</th>\n",
       "      <th>rare_auc</th>\n",
       "      <th>rare_ig</th>\n",
       "      <th>rare_cc</th>\n",
       "      <th>rare_sauc</th>\n",
       "      <th>rare_sim</th>\n",
       "      <th>qss_nss</th>\n",
       "      <th>qss_auc</th>\n",
       "      <th>qss_ig</th>\n",
       "      <th>qss_cc</th>\n",
       "      <th>qss_sauc</th>\n",
       "      <th>qss_sim</th>\n",
       "      <th>lds_nss</th>\n",
       "      <th>lds_auc</th>\n",
       "      <th>lds_ig</th>\n",
       "      <th>lds_cc</th>\n",
       "      <th>lds_sauc</th>\n",
       "      <th>lds_sim</th>\n",
       "      <th>imsig_nss</th>\n",
       "      <th>imsig_auc</th>\n",
       "      <th>imsig_ig</th>\n",
       "      <th>imsig_cc</th>\n",
       "      <th>imsig_sauc</th>\n",
       "      <th>imsig_sim</th>\n",
       "      <th>gbvs_nss</th>\n",
       "      <th>gbvs_auc</th>\n",
       "      <th>gbvs_ig</th>\n",
       "      <th>gbvs_cc</th>\n",
       "      <th>gbvs_sauc</th>\n",
       "      <th>gbvs_sim</th>\n",
       "      <th>gauss_nss</th>\n",
       "      <th>gauss_auc</th>\n",
       "      <th>gauss_ig</th>\n",
       "      <th>gauss_cc</th>\n",
       "      <th>gauss_sauc</th>\n",
       "      <th>gauss_sim</th>\n",
       "      <th>fes_nss</th>\n",
       "      <th>fes_auc</th>\n",
       "      <th>fes_ig</th>\n",
       "      <th>fes_cc</th>\n",
       "      <th>fes_sauc</th>\n",
       "      <th>fes_sim</th>\n",
       "      <th>dva_nss</th>\n",
       "      <th>dva_auc</th>\n",
       "      <th>dva_ig</th>\n",
       "      <th>dva_cc</th>\n",
       "      <th>dva_sauc</th>\n",
       "      <th>dva_sim</th>\n",
       "      <th>cvs_nss</th>\n",
       "      <th>cvs_auc</th>\n",
       "      <th>cvs_ig</th>\n",
       "      <th>cvs_cc</th>\n",
       "      <th>cvs_sauc</th>\n",
       "      <th>cvs_sim</th>\n",
       "      <th>cas_nss</th>\n",
       "      <th>cas_auc</th>\n",
       "      <th>cas_ig</th>\n",
       "      <th>cas_cc</th>\n",
       "      <th>cas_sauc</th>\n",
       "      <th>cas_sim</th>\n",
       "      <th>dgi_nss</th>\n",
       "      <th>dgi_auc</th>\n",
       "      <th>dgi_ig</th>\n",
       "      <th>dgi_cc</th>\n",
       "      <th>dgi_sauc</th>\n",
       "      <th>dgi_sim</th>\n",
       "      <th>dgiie_nss</th>\n",
       "      <th>dgiie_auc</th>\n",
       "      <th>dgiie_ig</th>\n",
       "      <th>dgiie_cc</th>\n",
       "      <th>dgiie_sauc</th>\n",
       "      <th>dgiie_sim</th>\n",
       "      <th>dgii_nss</th>\n",
       "      <th>dgii_auc</th>\n",
       "      <th>dgii_ig</th>\n",
       "      <th>dgii_cc</th>\n",
       "      <th>dgii_sauc</th>\n",
       "      <th>dgii_sim</th>\n",
       "      <th>icf_nss</th>\n",
       "      <th>icf_auc</th>\n",
       "      <th>icf_ig</th>\n",
       "      <th>icf_cc</th>\n",
       "      <th>icf_sauc</th>\n",
       "      <th>icf_sim</th>\n",
       "      <th>DoB</th>\n",
       "      <th>Gender</th>\n",
       "      <th>age_class</th>\n",
       "      <th>age_var</th>\n",
       "      <th>gender_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.021070e+14</td>\n",
       "      <td>[[288.8, 199.7], [1292.7, 915.9], [354.9, 946....</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0....</td>\n",
       "      <td>-0.173250</td>\n",
       "      <td>0.411120</td>\n",
       "      <td>-2.305416</td>\n",
       "      <td>-0.003296</td>\n",
       "      <td>0.422562</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>-0.068369</td>\n",
       "      <td>0.534746</td>\n",
       "      <td>-2.370930</td>\n",
       "      <td>-0.001242</td>\n",
       "      <td>0.473930</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.265845</td>\n",
       "      <td>0.619417</td>\n",
       "      <td>-2.088616</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>0.561867</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.034678</td>\n",
       "      <td>0.459067</td>\n",
       "      <td>-2.874031</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.422175</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.192551</td>\n",
       "      <td>0.566724</td>\n",
       "      <td>-2.148197</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>0.509269</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.021620</td>\n",
       "      <td>0.528534</td>\n",
       "      <td>-2.381335</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.424229</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.660138</td>\n",
       "      <td>-3.828059</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.049463</td>\n",
       "      <td>0.341909</td>\n",
       "      <td>-3.104079</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.423405</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>-0.054976</td>\n",
       "      <td>0.519950</td>\n",
       "      <td>-2.373077</td>\n",
       "      <td>-0.001234</td>\n",
       "      <td>0.485100</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>-0.032496</td>\n",
       "      <td>0.340028</td>\n",
       "      <td>-3.536221</td>\n",
       "      <td>-0.000658</td>\n",
       "      <td>0.351384</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>-0.010765</td>\n",
       "      <td>0.511411</td>\n",
       "      <td>-2.260876</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.450289</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>-0.070008</td>\n",
       "      <td>0.439027</td>\n",
       "      <td>-2.251289</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>0.396339</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>-0.070008</td>\n",
       "      <td>0.439027</td>\n",
       "      <td>-2.251289</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>0.396339</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>-0.070008</td>\n",
       "      <td>0.439027</td>\n",
       "      <td>-2.251289</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>0.396339</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.114301</td>\n",
       "      <td>0.507494</td>\n",
       "      <td>-2.188486</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.448937</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>1974</td>\n",
       "      <td>MALE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.021070e+14</td>\n",
       "      <td>[[891.4, 672.8], [1223.4, 744.5], [956.8, 167....</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0....</td>\n",
       "      <td>0.224671</td>\n",
       "      <td>0.538412</td>\n",
       "      <td>-2.087863</td>\n",
       "      <td>0.004922</td>\n",
       "      <td>0.560382</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.653525</td>\n",
       "      <td>0.666648</td>\n",
       "      <td>-1.880576</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>0.609916</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.130715</td>\n",
       "      <td>0.554148</td>\n",
       "      <td>-2.159101</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.502644</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.336710</td>\n",
       "      <td>0.562397</td>\n",
       "      <td>-2.283954</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.541136</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.316649</td>\n",
       "      <td>0.624198</td>\n",
       "      <td>-2.012104</td>\n",
       "      <td>0.006909</td>\n",
       "      <td>0.570362</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.470892</td>\n",
       "      <td>0.646396</td>\n",
       "      <td>-1.998717</td>\n",
       "      <td>0.010346</td>\n",
       "      <td>0.577086</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.070961</td>\n",
       "      <td>0.303732</td>\n",
       "      <td>-3.143107</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.553970</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.412778</td>\n",
       "      <td>0.484085</td>\n",
       "      <td>-2.451170</td>\n",
       "      <td>0.009019</td>\n",
       "      <td>0.511663</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.210373</td>\n",
       "      <td>0.570619</td>\n",
       "      <td>-2.095303</td>\n",
       "      <td>0.004617</td>\n",
       "      <td>0.554098</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.392331</td>\n",
       "      <td>0.461695</td>\n",
       "      <td>-2.707262</td>\n",
       "      <td>0.008586</td>\n",
       "      <td>0.598408</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.512079</td>\n",
       "      <td>0.644688</td>\n",
       "      <td>-1.937374</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>0.589819</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>-0.005878</td>\n",
       "      <td>0.477098</td>\n",
       "      <td>-2.172694</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>0.423363</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-0.005878</td>\n",
       "      <td>0.477098</td>\n",
       "      <td>-2.172694</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>0.423363</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>-0.005878</td>\n",
       "      <td>0.477098</td>\n",
       "      <td>-2.172694</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>0.423363</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.420799</td>\n",
       "      <td>0.630459</td>\n",
       "      <td>-2.100955</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.568972</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>1999</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.021070e+14</td>\n",
       "      <td>[[848.5, 851.7], [1400.1, 871.9], [1472.3, 295...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0....</td>\n",
       "      <td>0.686180</td>\n",
       "      <td>0.686619</td>\n",
       "      <td>-2.062538</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.714630</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.563947</td>\n",
       "      <td>0.671699</td>\n",
       "      <td>-2.040720</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>0.615217</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.399106</td>\n",
       "      <td>0.572741</td>\n",
       "      <td>-2.229954</td>\n",
       "      <td>0.008316</td>\n",
       "      <td>0.526731</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.146272</td>\n",
       "      <td>0.576055</td>\n",
       "      <td>-2.452639</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.558025</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.265811</td>\n",
       "      <td>0.599793</td>\n",
       "      <td>-2.210012</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.554732</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.292491</td>\n",
       "      <td>0.604644</td>\n",
       "      <td>-2.180548</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.516587</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>-0.312802</td>\n",
       "      <td>0.457577</td>\n",
       "      <td>-4.075178</td>\n",
       "      <td>-0.006476</td>\n",
       "      <td>0.461977</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.124923</td>\n",
       "      <td>0.350089</td>\n",
       "      <td>-2.576648</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.558574</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.601499</td>\n",
       "      <td>0.615484</td>\n",
       "      <td>-2.043944</td>\n",
       "      <td>0.012462</td>\n",
       "      <td>0.608590</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>-0.200191</td>\n",
       "      <td>0.424040</td>\n",
       "      <td>-3.434880</td>\n",
       "      <td>-0.004122</td>\n",
       "      <td>0.475200</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.428949</td>\n",
       "      <td>0.625828</td>\n",
       "      <td>-2.118001</td>\n",
       "      <td>0.008767</td>\n",
       "      <td>0.576280</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.310498</td>\n",
       "      <td>0.595997</td>\n",
       "      <td>-2.198153</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.554610</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.310498</td>\n",
       "      <td>0.595997</td>\n",
       "      <td>-2.198153</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.554610</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.310498</td>\n",
       "      <td>0.595997</td>\n",
       "      <td>-2.198153</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.554610</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.252469</td>\n",
       "      <td>0.566422</td>\n",
       "      <td>-2.277651</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>0.504005</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>1978</td>\n",
       "      <td>MALE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.021070e+14</td>\n",
       "      <td>[[908.6, 304.9], [238.5, 226.8], [1354.1, 827....</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0....</td>\n",
       "      <td>0.343402</td>\n",
       "      <td>0.564721</td>\n",
       "      <td>-2.492345</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0.584228</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.124839</td>\n",
       "      <td>0.608373</td>\n",
       "      <td>-2.529963</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.540472</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.469243</td>\n",
       "      <td>0.681522</td>\n",
       "      <td>-2.343746</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>0.634874</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>-0.156974</td>\n",
       "      <td>0.569886</td>\n",
       "      <td>-3.212336</td>\n",
       "      <td>-0.003233</td>\n",
       "      <td>0.452057</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>-0.053868</td>\n",
       "      <td>0.512905</td>\n",
       "      <td>-2.649902</td>\n",
       "      <td>-0.001103</td>\n",
       "      <td>0.458491</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.197219</td>\n",
       "      <td>0.559142</td>\n",
       "      <td>-2.674669</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.479845</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>-0.304917</td>\n",
       "      <td>0.241814</td>\n",
       "      <td>-4.429585</td>\n",
       "      <td>-0.006338</td>\n",
       "      <td>0.381134</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>-0.421048</td>\n",
       "      <td>0.272418</td>\n",
       "      <td>-3.903908</td>\n",
       "      <td>-0.008742</td>\n",
       "      <td>0.380331</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.201546</td>\n",
       "      <td>0.572828</td>\n",
       "      <td>-2.587253</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.560383</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>-0.299566</td>\n",
       "      <td>0.446550</td>\n",
       "      <td>-4.007713</td>\n",
       "      <td>-0.006180</td>\n",
       "      <td>0.467648</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.133272</td>\n",
       "      <td>0.488155</td>\n",
       "      <td>-2.804374</td>\n",
       "      <td>-0.002767</td>\n",
       "      <td>0.428650</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.306222</td>\n",
       "      <td>0.607256</td>\n",
       "      <td>-2.453991</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.577337</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.306222</td>\n",
       "      <td>0.607256</td>\n",
       "      <td>-2.453991</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.577337</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.306222</td>\n",
       "      <td>0.607256</td>\n",
       "      <td>-2.453991</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.577337</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.194748</td>\n",
       "      <td>0.559170</td>\n",
       "      <td>-2.561761</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.492652</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>1994</td>\n",
       "      <td>MALE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.021070e+14</td>\n",
       "      <td>[[921.4, 688.4], [117.9, 985.6], [211.4, 463.1...</td>\n",
       "      <td>[[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0....</td>\n",
       "      <td>-0.083830</td>\n",
       "      <td>0.428131</td>\n",
       "      <td>-2.281806</td>\n",
       "      <td>-0.002085</td>\n",
       "      <td>0.434062</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.105079</td>\n",
       "      <td>0.564803</td>\n",
       "      <td>-2.294462</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.505519</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.515887</td>\n",
       "      <td>-2.300617</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.457487</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.011024</td>\n",
       "      <td>0.425366</td>\n",
       "      <td>-2.958935</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.391305</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>-0.028338</td>\n",
       "      <td>0.506887</td>\n",
       "      <td>-2.279923</td>\n",
       "      <td>-0.000619</td>\n",
       "      <td>0.438278</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-0.046097</td>\n",
       "      <td>0.492266</td>\n",
       "      <td>-2.528424</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>0.398639</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.529895</td>\n",
       "      <td>-3.555503</td>\n",
       "      <td>0.003184</td>\n",
       "      <td>0.460544</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.185401</td>\n",
       "      <td>0.559583</td>\n",
       "      <td>-2.870767</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>0.473137</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>-0.020497</td>\n",
       "      <td>0.498233</td>\n",
       "      <td>-2.469421</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>0.480668</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.244666</td>\n",
       "      <td>0.454885</td>\n",
       "      <td>-3.284751</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.409166</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.103539</td>\n",
       "      <td>0.508779</td>\n",
       "      <td>-2.280448</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>0.449193</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>-0.063595</td>\n",
       "      <td>0.463441</td>\n",
       "      <td>-2.258641</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>0.414212</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.063595</td>\n",
       "      <td>0.463441</td>\n",
       "      <td>-2.258641</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>0.414212</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.063595</td>\n",
       "      <td>0.463441</td>\n",
       "      <td>-2.258641</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>0.414212</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.011351</td>\n",
       "      <td>0.486064</td>\n",
       "      <td>-2.204089</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.421690</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>1975</td>\n",
       "      <td>MALE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            id  \\\n",
       "0           0  2.021070e+14   \n",
       "1           1  2.021070e+14   \n",
       "2           2  2.021070e+14   \n",
       "3           3  2.021070e+14   \n",
       "4           4  2.021070e+14   \n",
       "\n",
       "                                           fixations  \\\n",
       "0  [[288.8, 199.7], [1292.7, 915.9], [354.9, 946....   \n",
       "1  [[891.4, 672.8], [1223.4, 744.5], [956.8, 167....   \n",
       "2  [[848.5, 851.7], [1400.1, 871.9], [1472.3, 295...   \n",
       "3  [[908.6, 304.9], [238.5, 226.8], [1354.1, 827....   \n",
       "4  [[921.4, 688.4], [117.9, 985.6], [211.4, 463.1...   \n",
       "\n",
       "                                              fixmap   aim_nss   aim_auc  \\\n",
       "0  [[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0.... -0.173250  0.411120   \n",
       "1  [[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0....  0.224671  0.538412   \n",
       "2  [[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0....  0.686180  0.686619   \n",
       "3  [[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0....  0.343402  0.564721   \n",
       "4  [[0. 0. 0. ... 0. 0. 0.]\\n [0. 0. 0. ... 0. 0.... -0.083830  0.428131   \n",
       "\n",
       "     aim_ig    aim_cc  aim_sauc   aim_sim  rare_nss  rare_auc   rare_ig  \\\n",
       "0 -2.305416 -0.003296  0.422562  0.000405 -0.068369  0.534746 -2.370930   \n",
       "1 -2.087863  0.004922  0.560382  0.000518  0.653525  0.666648 -1.880576   \n",
       "2 -2.062538  0.014210  0.714630  0.000539  0.563947  0.671699 -2.040720   \n",
       "3 -2.492345  0.006796  0.584228  0.000482  0.124839  0.608373 -2.529963   \n",
       "4 -2.281806 -0.002085  0.434062  0.000555  0.105079  0.564803 -2.294462   \n",
       "\n",
       "    rare_cc  rare_sauc  rare_sim   qss_nss   qss_auc    qss_ig    qss_cc  \\\n",
       "0 -0.001242   0.473930  0.000409  0.265845  0.619417 -2.088616  0.005430   \n",
       "1  0.013930   0.609916  0.000726  0.130715  0.554148 -2.159101  0.002810   \n",
       "2  0.011478   0.615217  0.000624  0.399106  0.572741 -2.229954  0.008316   \n",
       "3  0.002413   0.540472  0.000471  0.469243  0.681522 -2.343746  0.009732   \n",
       "4  0.002795   0.505519  0.000628  0.001662  0.515887 -2.300617  0.000033   \n",
       "\n",
       "   qss_sauc   qss_sim   lds_nss   lds_auc    lds_ig    lds_cc  lds_sauc  \\\n",
       "0  0.561867  0.000496  0.034678  0.459067 -2.874031  0.000726  0.422175   \n",
       "1  0.502644  0.000514  0.336710  0.562397 -2.283954  0.007330  0.541136   \n",
       "2  0.526731  0.000532  0.146272  0.576055 -2.452639  0.003002  0.558025   \n",
       "3  0.634874  0.000549 -0.156974  0.569886 -3.212336 -0.003233  0.452057   \n",
       "4  0.457487  0.000574  0.011024  0.425366 -2.958935  0.000246  0.391305   \n",
       "\n",
       "    lds_sim  imsig_nss  imsig_auc  imsig_ig  imsig_cc  imsig_sauc  imsig_sim  \\\n",
       "0  0.000450   0.192551   0.566724 -2.148197  0.004010    0.509269   0.000471   \n",
       "1  0.000693   0.316649   0.624198 -2.012104  0.006909    0.570362   0.000552   \n",
       "2  0.000514   0.265811   0.599793 -2.210012  0.005515    0.554732   0.000486   \n",
       "3  0.000340  -0.053868   0.512905 -2.649902 -0.001103    0.458491   0.000419   \n",
       "4  0.000581  -0.028338   0.506887 -2.279923 -0.000619    0.438278   0.000566   \n",
       "\n",
       "   gbvs_nss  gbvs_auc   gbvs_ig   gbvs_cc  gbvs_sauc  gbvs_sim  gauss_nss  \\\n",
       "0  0.021620  0.528534 -2.381335  0.000444   0.424229  0.000437   0.002620   \n",
       "1  0.470892  0.646396 -1.998717  0.010346   0.577086  0.000649   0.070961   \n",
       "2  0.292491  0.604644 -2.180548  0.006040   0.516587  0.000525  -0.312802   \n",
       "3  0.197219  0.559142 -2.674669  0.004029   0.479845  0.000493  -0.304917   \n",
       "4 -0.046097  0.492266 -2.528424 -0.001102   0.398639  0.000553   0.132075   \n",
       "\n",
       "   gauss_auc  gauss_ig  gauss_cc  gauss_sauc  gauss_sim   fes_nss   fes_auc  \\\n",
       "0   0.660138 -3.828059  0.000004    0.436100   0.000430  0.049463  0.341909   \n",
       "1   0.303732 -3.143107  0.001537    0.553970   0.000567  0.412778  0.484085   \n",
       "2   0.457577 -4.075178 -0.006476    0.461977   0.000075  0.124923  0.350089   \n",
       "3   0.241814 -4.429585 -0.006338    0.381134   0.000083 -0.421048  0.272418   \n",
       "4   0.529895 -3.555503  0.003184    0.460544   0.000775  0.185401  0.559583   \n",
       "\n",
       "     fes_ig    fes_cc  fes_sauc   fes_sim   dva_nss   dva_auc    dva_ig  \\\n",
       "0 -3.104079  0.001031  0.423405  0.000460 -0.054976  0.519950 -2.373077   \n",
       "1 -2.451170  0.009019  0.511663  0.000755  0.210373  0.570619 -2.095303   \n",
       "2 -2.576648  0.002628  0.558574  0.000507  0.601499  0.615484 -2.043944   \n",
       "3 -3.903908 -0.008742  0.380331  0.000175  0.201546  0.572828 -2.587253   \n",
       "4 -2.870767  0.004453  0.473137  0.000723 -0.020497  0.498233 -2.469421   \n",
       "\n",
       "     dva_cc  dva_sauc   dva_sim   cvs_nss   cvs_auc    cvs_ig    cvs_cc  \\\n",
       "0 -0.001234  0.485100  0.000403 -0.032496  0.340028 -3.536221 -0.000658   \n",
       "1  0.004617  0.554098  0.000584  0.392331  0.461695 -2.707262  0.008586   \n",
       "2  0.012462  0.608590  0.000702 -0.200191  0.424040 -3.434880 -0.004122   \n",
       "3  0.004169  0.560383  0.000521 -0.299566  0.446550 -4.007713 -0.006180   \n",
       "4 -0.000489  0.480668  0.000561  0.244666  0.454885 -3.284751  0.005825   \n",
       "\n",
       "   cvs_sauc   cvs_sim   cas_nss   cas_auc    cas_ig    cas_cc  cas_sauc  \\\n",
       "0  0.351384  0.000398 -0.010765  0.511411 -2.260876 -0.000151  0.450289   \n",
       "1  0.598408  0.000921  0.512079  0.644688 -1.937374  0.011140  0.589819   \n",
       "2  0.475200  0.000228  0.428949  0.625828 -2.118001  0.008767  0.576280   \n",
       "3  0.467648  0.000128 -0.133272  0.488155 -2.804374 -0.002767  0.428650   \n",
       "4  0.409166  0.000902  0.103539  0.508779 -2.280448  0.002328  0.449193   \n",
       "\n",
       "    cas_sim   dgi_nss   dgi_auc    dgi_ig    dgi_cc  dgi_sauc   dgi_sim  \\\n",
       "0  0.000428 -0.070008  0.439027 -2.251289 -0.001458  0.396339  0.000418   \n",
       "1  0.000622 -0.005878  0.477098 -2.172694 -0.000254  0.423363  0.000476   \n",
       "2  0.000537  0.310498  0.595997 -2.198153  0.006332  0.554610  0.000482   \n",
       "3  0.000396  0.306222  0.607256 -2.453991  0.006450  0.577337  0.000483   \n",
       "4  0.000606 -0.063595  0.463441 -2.258641 -0.001421  0.414212  0.000560   \n",
       "\n",
       "   dgiie_nss  dgiie_auc  dgiie_ig  dgiie_cc  dgiie_sauc  dgiie_sim  dgii_nss  \\\n",
       "0  -0.070008   0.439027 -2.251289 -0.001458    0.396339   0.000418 -0.070008   \n",
       "1  -0.005878   0.477098 -2.172694 -0.000254    0.423363   0.000476 -0.005878   \n",
       "2   0.310498   0.595997 -2.198153  0.006332    0.554610   0.000482  0.310498   \n",
       "3   0.306222   0.607256 -2.453991  0.006450    0.577337   0.000483  0.306222   \n",
       "4  -0.063595   0.463441 -2.258641 -0.001421    0.414212   0.000560 -0.063595   \n",
       "\n",
       "   dgii_auc   dgii_ig   dgii_cc  dgii_sauc  dgii_sim   icf_nss   icf_auc  \\\n",
       "0  0.439027 -2.251289 -0.001458   0.396339  0.000418  0.114301  0.507494   \n",
       "1  0.477098 -2.172694 -0.000254   0.423363  0.000476  0.420799  0.630459   \n",
       "2  0.595997 -2.198153  0.006332   0.554610  0.000482  0.252469  0.566422   \n",
       "3  0.607256 -2.453991  0.006450   0.577337  0.000483  0.194748  0.559170   \n",
       "4  0.463441 -2.258641 -0.001421   0.414212  0.000560  0.011351  0.486064   \n",
       "\n",
       "     icf_ig    icf_cc  icf_sauc   icf_sim   DoB  Gender  age_class  age_var  \\\n",
       "0 -2.188486  0.002919  0.448937  0.000436  1974    MALE        0.0        0   \n",
       "1 -2.100955  0.009112  0.568972  0.000498  1999  FEMALE        2.0        2   \n",
       "2 -2.277651  0.005538  0.504005  0.000442  1978    MALE        0.0        0   \n",
       "3 -2.561761  0.003959  0.492652  0.000439  1994    MALE        1.0        1   \n",
       "4 -2.204089  0.000274  0.421690  0.000574  1975    MALE        0.0        0   \n",
       "\n",
       "   gender_var  \n",
       "0           1  \n",
       "1           0  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "423c66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in feature selection info\n",
    "df_age_cols = pd.read_csv('Data/age_selectk.csv')\n",
    "df_sex_cols = pd.read_csv('Data/sex_selectk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34525ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>col_name</th>\n",
       "      <th>fs_score</th>\n",
       "      <th>map</th>\n",
       "      <th>avg_fs_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66</td>\n",
       "      <td>qss_nss</td>\n",
       "      <td>1.948164</td>\n",
       "      <td>qss</td>\n",
       "      <td>1.940919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>qss_auc</td>\n",
       "      <td>2.069283</td>\n",
       "      <td>qss</td>\n",
       "      <td>1.940919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>qss_ig</td>\n",
       "      <td>1.968535</td>\n",
       "      <td>qss</td>\n",
       "      <td>1.940919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "      <td>qss_cc</td>\n",
       "      <td>2.299368</td>\n",
       "      <td>qss</td>\n",
       "      <td>1.940919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>qss_sauc</td>\n",
       "      <td>2.042680</td>\n",
       "      <td>qss</td>\n",
       "      <td>1.940919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  col_name  fs_score  map  avg_fs_score\n",
       "0          66   qss_nss  1.948164  qss      1.940919\n",
       "1          67   qss_auc  2.069283  qss      1.940919\n",
       "2          68    qss_ig  1.968535  qss      1.940919\n",
       "3          69    qss_cc  2.299368  qss      1.940919\n",
       "4          70  qss_sauc  2.042680  qss      1.940919"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_age_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1053ee",
   "metadata": {},
   "source": [
    "## Age CNN with 15 variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfb3332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select top 15 age columns from feature selection\n",
    "temp = df_age_cols.nlargest(15,'fs_score')\n",
    "cols = temp['col_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d11b3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_metrics[cols], df_metrics.age_class, test_size=0.2,random_state=131) # 70% training and 30% test\n",
    "y_train = le.fit_transform(y_train)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63d1b904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 23.41% (7.01%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.17      0.33      0.22        21\n",
      "         1.0       0.38      0.24      0.30        33\n",
      "         2.0       0.34      0.48      0.40        31\n",
      "         3.0       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.28       107\n",
      "   macro avg       0.22      0.26      0.23       107\n",
      "weighted avg       0.25      0.28      0.25       107\n",
      "\n",
      "Model F1 Score  0.2803738317757009\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(15, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#cross validate model on training data\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "#predict test data and print classification results\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131df2a",
   "metadata": {},
   "source": [
    "## Same data different CNN architecture - 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecb21b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 23.83% (7.86%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.23      0.48      0.31        21\n",
      "         1.0       0.35      0.24      0.29        33\n",
      "         2.0       0.31      0.39      0.34        31\n",
      "         3.0       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.28       107\n",
      "   macro avg       0.22      0.28      0.23       107\n",
      "weighted avg       0.24      0.28      0.25       107\n",
      "\n",
      "Model F1 Score  0.2803738317757009\n"
     ]
    }
   ],
   "source": [
    "#repeat process, test new model architecture\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(15, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(15, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(10, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f44f7f",
   "metadata": {},
   "source": [
    "## age cnn with features from logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "215beb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select features used in logistic regression age model\n",
    "temp = df_age_cols.nlargest(50,'fs_score')\n",
    "cols = temp['col_name'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_metrics[cols], df_metrics.age_class, test_size=0.2,random_state=131) # 70% training and 30% test\n",
    "y_train = le.fit_transform(y_train)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "feat_names = ['3', '4', '12', '16', '17', '20', '34', '49']\n",
    "X_age = np.take(X_train, feat_names, axis = 1)\n",
    "X_test = np.take(X_test, feat_names, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62e9bfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 26.24% (6.04%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.48      0.34        21\n",
      "         1.0       0.19      0.09      0.12        33\n",
      "         2.0       0.38      0.65      0.48        31\n",
      "         3.0       1.00      0.05      0.09        22\n",
      "\n",
      "    accuracy                           0.32       107\n",
      "   macro avg       0.46      0.31      0.26       107\n",
      "weighted avg       0.43      0.32      0.26       107\n",
      "\n",
      "Model F1 Score  0.3177570093457944\n"
     ]
    }
   ],
   "source": [
    "#test model with this data and print results\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_age, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_age, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378282f7",
   "metadata": {},
   "source": [
    "## hypertune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c241eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd6bc65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for input to bayesian optimisation function\n",
    "\n",
    "def nn_cl_bo(activation, optimizer, learning_rate,  batch_size, epochs): #,normalization, dropout, dropout_rate):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    #neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    #define network architecture\n",
    "    def nn_cl_fun():\n",
    "        opt = Adam(lr = learning_rate)\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(15, input_dim=8, activation=activation))\n",
    "        nn.add(Dense(15, activation=activation))\n",
    "        nn.add(Dense(4, activation='softmax'))\n",
    "        nn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=141)\n",
    "    score = cross_val_score(nn, X_age, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a79605e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for optimisation\n",
    "params_nn ={\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 0.5),\n",
    "    'batch_size':(20, 100),\n",
    "    'epochs':(5, 100)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be35a43e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  epochs   | learni... | optimizer |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.2388  \u001b[0m | \u001b[0m 8.177   \u001b[0m | \u001b[0m 40.64   \u001b[0m | \u001b[0m 88.38   \u001b[0m | \u001b[0m 0.3721  \u001b[0m | \u001b[0m 4.887   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.2717  \u001b[0m | \u001b[95m 4.655   \u001b[0m | \u001b[95m 96.17   \u001b[0m | \u001b[95m 91.8    \u001b[0m | \u001b[95m 0.04831 \u001b[0m | \u001b[95m 5.476   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.2904  \u001b[0m | \u001b[95m 1.023   \u001b[0m | \u001b[95m 71.27   \u001b[0m | \u001b[95m 12.58   \u001b[0m | \u001b[95m 0.1237  \u001b[0m | \u001b[95m 2.702   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.2833  \u001b[0m | \u001b[0m 2.131   \u001b[0m | \u001b[0m 99.87   \u001b[0m | \u001b[0m 41.99   \u001b[0m | \u001b[0m 0.3647  \u001b[0m | \u001b[0m 3.162   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.2411  \u001b[0m | \u001b[0m 4.292   \u001b[0m | \u001b[0m 63.62   \u001b[0m | \u001b[0m 46.12   \u001b[0m | \u001b[0m 0.299   \u001b[0m | \u001b[0m 0.7348  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.262   \u001b[0m | \u001b[0m 7.75    \u001b[0m | \u001b[0m 44.64   \u001b[0m | \u001b[0m 51.24   \u001b[0m | \u001b[0m 0.1129  \u001b[0m | \u001b[0m 4.341   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.2435  \u001b[0m | \u001b[0m 3.072   \u001b[0m | \u001b[0m 59.57   \u001b[0m | \u001b[0m 34.19   \u001b[0m | \u001b[0m 0.2146  \u001b[0m | \u001b[0m 5.321   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.2787  \u001b[0m | \u001b[0m 0.3685  \u001b[0m | \u001b[0m 59.99   \u001b[0m | \u001b[0m 82.21   \u001b[0m | \u001b[0m 0.265   \u001b[0m | \u001b[0m 3.021   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.2646  \u001b[0m | \u001b[0m 6.648   \u001b[0m | \u001b[0m 79.43   \u001b[0m | \u001b[0m 88.68   \u001b[0m | \u001b[0m 0.4914  \u001b[0m | \u001b[0m 1.539   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.2877  \u001b[0m | \u001b[0m 0.8396  \u001b[0m | \u001b[0m 31.66   \u001b[0m | \u001b[0m 44.03   \u001b[0m | \u001b[0m 0.1855  \u001b[0m | \u001b[0m 6.802   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.2529  \u001b[0m | \u001b[0m 5.915   \u001b[0m | \u001b[0m 73.74   \u001b[0m | \u001b[0m 75.31   \u001b[0m | \u001b[0m 0.3222  \u001b[0m | \u001b[0m 6.282   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.2669  \u001b[0m | \u001b[0m 2.872   \u001b[0m | \u001b[0m 29.45   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 0.06043 \u001b[0m | \u001b[0m 6.801   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.2671  \u001b[0m | \u001b[0m 4.17    \u001b[0m | \u001b[0m 37.09   \u001b[0m | \u001b[0m 48.14   \u001b[0m | \u001b[0m 0.4997  \u001b[0m | \u001b[0m 0.6926  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.2764  \u001b[0m | \u001b[0m 2.136   \u001b[0m | \u001b[0m 30.75   \u001b[0m | \u001b[0m 73.84   \u001b[0m | \u001b[0m 0.1781  \u001b[0m | \u001b[0m 0.6172  \u001b[0m |\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m 0.2906  \u001b[0m | \u001b[95m 7.524   \u001b[0m | \u001b[95m 29.53   \u001b[0m | \u001b[95m 8.327   \u001b[0m | \u001b[95m 0.099   \u001b[0m | \u001b[95m 6.809   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.2858  \u001b[0m | \u001b[0m 1.918   \u001b[0m | \u001b[0m 60.04   \u001b[0m | \u001b[0m 84.89   \u001b[0m | \u001b[0m 0.2352  \u001b[0m | \u001b[0m 5.513   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.2787  \u001b[0m | \u001b[0m 1.734   \u001b[0m | \u001b[0m 36.45   \u001b[0m | \u001b[0m 90.65   \u001b[0m | \u001b[0m 0.3178  \u001b[0m | \u001b[0m 2.425   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.2603  \u001b[0m | \u001b[0m 4.153   \u001b[0m | \u001b[0m 68.47   \u001b[0m | \u001b[0m 20.29   \u001b[0m | \u001b[0m 0.3657  \u001b[0m | \u001b[0m 1.119   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.2434  \u001b[0m | \u001b[0m 3.934   \u001b[0m | \u001b[0m 46.59   \u001b[0m | \u001b[0m 40.22   \u001b[0m | \u001b[0m 0.2809  \u001b[0m | \u001b[0m 1.295   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.2506  \u001b[0m | \u001b[0m 2.772   \u001b[0m | \u001b[0m 27.34   \u001b[0m | \u001b[0m 78.75   \u001b[0m | \u001b[0m 0.3364  \u001b[0m | \u001b[0m 2.796   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.2647  \u001b[0m | \u001b[0m 2.339   \u001b[0m | \u001b[0m 70.19   \u001b[0m | \u001b[0m 86.32   \u001b[0m | \u001b[0m 0.2372  \u001b[0m | \u001b[0m 2.086   \u001b[0m |\n",
      "| \u001b[95m 22      \u001b[0m | \u001b[95m 0.2951  \u001b[0m | \u001b[95m 1.403   \u001b[0m | \u001b[95m 57.62   \u001b[0m | \u001b[95m 28.2    \u001b[0m | \u001b[95m 0.1839  \u001b[0m | \u001b[95m 3.389   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.267   \u001b[0m | \u001b[0m 8.067   \u001b[0m | \u001b[0m 35.12   \u001b[0m | \u001b[0m 16.02   \u001b[0m | \u001b[0m 0.1429  \u001b[0m | \u001b[0m 1.028   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.2621  \u001b[0m | \u001b[0m 5.537   \u001b[0m | \u001b[0m 96.07   \u001b[0m | \u001b[0m 99.04   \u001b[0m | \u001b[0m 0.1779  \u001b[0m | \u001b[0m 5.295   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.2787  \u001b[0m | \u001b[0m 2.181   \u001b[0m | \u001b[0m 71.83   \u001b[0m | \u001b[0m 28.65   \u001b[0m | \u001b[0m 0.2402  \u001b[0m | \u001b[0m 4.317   \u001b[0m |\n",
      "| \u001b[95m 26      \u001b[0m | \u001b[95m 0.2953  \u001b[0m | \u001b[95m 0.895   \u001b[0m | \u001b[95m 56.85   \u001b[0m | \u001b[95m 28.64   \u001b[0m | \u001b[95m 0.04216 \u001b[0m | \u001b[95m 1.506   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.2811  \u001b[0m | \u001b[0m 0.6524  \u001b[0m | \u001b[0m 54.51   \u001b[0m | \u001b[0m 27.6    \u001b[0m | \u001b[0m 0.1854  \u001b[0m | \u001b[0m 3.79    \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.2834  \u001b[0m | \u001b[0m 2.488   \u001b[0m | \u001b[0m 58.21   \u001b[0m | \u001b[0m 25.38   \u001b[0m | \u001b[0m 0.3072  \u001b[0m | \u001b[0m 1.477   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.2764  \u001b[0m | \u001b[0m 0.3342  \u001b[0m | \u001b[0m 59.19   \u001b[0m | \u001b[0m 27.72   \u001b[0m | \u001b[0m 0.16    \u001b[0m | \u001b[0m 0.2157  \u001b[0m |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72cbc123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'sigmoid',\n",
       " 'batch_size': 56.850471617136726,\n",
       " 'epochs': 28.642656540450194,\n",
       " 'learning_rate': 0.042162474282075285,\n",
       " 'optimizer': 1.5060934987503525}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract max parameters\n",
    "params_nn_ = nn_bo.max['params']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf6df1",
   "metadata": {},
   "source": [
    "## Age data with hypertuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e17d3994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 28.92% (10.77%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.43      0.32        21\n",
      "         1.0       0.30      0.42      0.35        33\n",
      "         2.0       0.53      0.29      0.38        31\n",
      "         3.0       0.43      0.14      0.21        22\n",
      "\n",
      "    accuracy                           0.33       107\n",
      "   macro avg       0.38      0.32      0.31       107\n",
      "weighted avg       0.38      0.33      0.32       107\n",
      "\n",
      "Model F1 Score  0.32710280373831774\n"
     ]
    }
   ],
   "source": [
    "#create model with tuned paramters\n",
    "def baseline_model():\n",
    "    opt = Adam(lr = 0.04)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=8, activation='sigmoid'))\n",
    "    model.add(Dense(15, activation='sigmoid'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=29, batch_size=56, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=20, shuffle=True)\n",
    "results = cross_val_score(estimator, X_age, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_age, y_train, epochs=29, batch_size=56, verbose=0)\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd46f2af",
   "metadata": {},
   "source": [
    "# SEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104a4f8",
   "metadata": {},
   "source": [
    "## Sex CNN with 15 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a9c8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract top 15 features for sex after feature selection\n",
    "temp = df_sex_cols.nlargest(15,'fs_score')\n",
    "cols_sex = temp['col_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8ed7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_metrics[cols_sex], df_metrics.gender_var, test_size=0.2,random_state=131) # 70% training and 30% test\n",
    "y_train = le.fit_transform(y_train)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ccc57aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 49.17% (6.50%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.32      0.39        53\n",
      "           1       0.50      0.67      0.57        54\n",
      "\n",
      "    accuracy                           0.50       107\n",
      "   macro avg       0.49      0.49      0.48       107\n",
      "weighted avg       0.49      0.50      0.48       107\n",
      "\n",
      "Model F1 Score  0.4953271028037383\n"
     ]
    }
   ],
   "source": [
    "#define initial model & classify test data\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(15, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c10023",
   "metadata": {},
   "source": [
    "## Same data different architecture - 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ccf9cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 48.00% (6.29%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.26      0.35        53\n",
      "           1       0.52      0.78      0.62        54\n",
      "\n",
      "    accuracy                           0.52       107\n",
      "   macro avg       0.53      0.52      0.49       107\n",
      "weighted avg       0.53      0.52      0.49       107\n",
      "\n",
      "Model F1 Score  0.5233644859813084\n"
     ]
    }
   ],
   "source": [
    "#use same data with different network architecture\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(15, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(15, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(10, input_dim=15, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c3d97",
   "metadata": {},
   "source": [
    "## Sex data with features from logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56fb295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test network using same data as was selected for logistic regression\n",
    "temp = df_sex_cols.nlargest(50,'fs_score')\n",
    "cols_sex = temp['col_name'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_metrics[cols_sex], df_metrics.gender_var, test_size=0.2,random_state=131) # 70% training and 30% test\n",
    "y_train = le.fit_transform(y_train)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "feat_names = ['0', '3', '20', '32', '33', '36', '38']\n",
    "X_sex = np.take(X_train, feat_names, axis = 1)\n",
    "X_test = np.take(X_test, feat_names, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71b1b8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 49.45% (6.69%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.36      0.42        53\n",
      "           1       0.51      0.67      0.58        54\n",
      "\n",
      "    accuracy                           0.51       107\n",
      "   macro avg       0.51      0.51      0.50       107\n",
      "weighted avg       0.51      0.51      0.50       107\n",
      "\n",
      "Model F1 Score  0.514018691588785\n"
     ]
    }
   ],
   "source": [
    "#use this data in network with 1 hidden layer & classify test data\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=7, activation='relu'))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_sex, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_sex, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1d894",
   "metadata": {},
   "source": [
    "## Same data with 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca87438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 52.21% (6.45%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.23      0.31        53\n",
      "           1       0.51      0.78      0.61        54\n",
      "\n",
      "    accuracy                           0.50       107\n",
      "   macro avg       0.50      0.50      0.46       107\n",
      "weighted avg       0.50      0.50      0.46       107\n",
      "\n",
      "Model F1 Score  0.5046728971962616\n"
     ]
    }
   ],
   "source": [
    "#use same data in network with 3 hidden layers\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_dim=7, activation='relu'))\n",
    "    model.add(Dense(7, activation='relu'))\n",
    "    model.add(Dense(7, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_sex, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_sex, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c24b21",
   "metadata": {},
   "source": [
    "## Sex data with top 30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bdec1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract top 30 features ranked by anova correlation\n",
    "temp = df_sex_cols.nlargest(30,'fs_score')\n",
    "cols_sex = temp['col_name'].tolist()\n",
    "\n",
    "#split data into train and test using only these 30 features\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_metrics[cols_sex], df_metrics.gender_var, test_size=0.2,random_state=131) # 70% training and 30% test\n",
    "y_train = le.fit_transform(y_train)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9c1bd78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 49.22% (7.61%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.43      0.45        53\n",
      "           1       0.47      0.50      0.49        54\n",
      "\n",
      "    accuracy                           0.47       107\n",
      "   macro avg       0.47      0.47      0.47       107\n",
      "weighted avg       0.47      0.47      0.47       107\n",
      "\n",
      "Model F1 Score  0.4672897196261683\n"
     ]
    }
   ],
   "source": [
    "#create model with 1 hidden layer using this data to train\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=30, activation='relu'))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "#predict test data\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c4079",
   "metadata": {},
   "source": [
    "## same data 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88cf656f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 45.87% (5.20%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.38      0.43        53\n",
      "           1       0.50      0.61      0.55        54\n",
      "\n",
      "    accuracy                           0.50       107\n",
      "   macro avg       0.49      0.49      0.49       107\n",
      "weighted avg       0.49      0.50      0.49       107\n",
      "\n",
      "Model F1 Score  0.4953271028037383\n"
     ]
    }
   ],
   "source": [
    "#model with 3 hidden layers & print results\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=30, activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a91c4",
   "metadata": {},
   "source": [
    "## Hypertune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a8c447de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract top 50 features ranked by anova correlation\n",
    "temp = df_sex_cols.nlargest(50,'fs_score')\n",
    "cols_sex = temp['col_name'].tolist()\n",
    "\n",
    "#split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_metrics[cols_sex], df_metrics.gender_var, test_size=0.2,random_state=131) # 70% training and 30% test\n",
    "y_train = le.fit_transform(y_train)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "#feature names used to train logistic regression models in sex, filter to only include these columns\n",
    "feat_names = ['0', '3', '20', '32', '33', '36', '38']\n",
    "X_sex = np.take(X_train, feat_names, axis = 1)\n",
    "X_test = np.take(X_test, feat_names, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8d5aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for input to bayesian optimisation function\n",
    "def nn_cl_bo(activation, optimizer, learning_rate,  batch_size, epochs): #,normalization, dropout, dropout_rate):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    #neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    #define network - 3 hidden layers is best\n",
    "    def nn_cl_fun():\n",
    "        opt = Adam(lr = learning_rate)\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(7, input_dim=7, activation=activation))\n",
    "        nn.add(Dense(7, activation=activation))\n",
    "        nn.add(Dense(7, activation=activation))\n",
    "        nn.add(Dense(4, activation=activation))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=141)\n",
    "    score = cross_val_score(nn, X_sex, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2987b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters for optimisation\n",
    "params_nn ={\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 0.5),\n",
    "    'batch_size':(20, 100),\n",
    "    'epochs':(5, 100)\n",
    "}\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "63560627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  epochs   | learni... | optimizer |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5317  \u001b[0m | \u001b[0m 8.177   \u001b[0m | \u001b[0m 40.64   \u001b[0m | \u001b[0m 88.38   \u001b[0m | \u001b[0m 0.3721  \u001b[0m | \u001b[0m 4.887   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5202  \u001b[0m | \u001b[0m 4.655   \u001b[0m | \u001b[0m 96.17   \u001b[0m | \u001b[0m 91.8    \u001b[0m | \u001b[0m 0.04831 \u001b[0m | \u001b[0m 5.476   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5525  \u001b[0m | \u001b[95m 1.023   \u001b[0m | \u001b[95m 71.27   \u001b[0m | \u001b[95m 12.58   \u001b[0m | \u001b[95m 0.1237  \u001b[0m | \u001b[95m 2.702   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5457  \u001b[0m | \u001b[0m 2.131   \u001b[0m | \u001b[0m 99.87   \u001b[0m | \u001b[0m 41.99   \u001b[0m | \u001b[0m 0.3647  \u001b[0m | \u001b[0m 3.162   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5339  \u001b[0m | \u001b[0m 4.292   \u001b[0m | \u001b[0m 63.62   \u001b[0m | \u001b[0m 46.12   \u001b[0m | \u001b[0m 0.299   \u001b[0m | \u001b[0m 0.7348  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5245  \u001b[0m | \u001b[0m 7.75    \u001b[0m | \u001b[0m 44.64   \u001b[0m | \u001b[0m 51.24   \u001b[0m | \u001b[0m 0.1129  \u001b[0m | \u001b[0m 4.341   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5297  \u001b[0m | \u001b[0m 3.072   \u001b[0m | \u001b[0m 59.57   \u001b[0m | \u001b[0m 34.19   \u001b[0m | \u001b[0m 0.2146  \u001b[0m | \u001b[0m 5.321   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.3685  \u001b[0m | \u001b[0m 59.99   \u001b[0m | \u001b[0m 82.21   \u001b[0m | \u001b[0m 0.265   \u001b[0m | \u001b[0m 3.021   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.452   \u001b[0m | \u001b[0m 6.648   \u001b[0m | \u001b[0m 79.43   \u001b[0m | \u001b[0m 88.68   \u001b[0m | \u001b[0m 0.4914  \u001b[0m | \u001b[0m 1.539   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5503  \u001b[0m | \u001b[0m 0.8396  \u001b[0m | \u001b[0m 31.66   \u001b[0m | \u001b[0m 44.03   \u001b[0m | \u001b[0m 0.1855  \u001b[0m | \u001b[0m 6.802   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.5387  \u001b[0m | \u001b[0m 5.915   \u001b[0m | \u001b[0m 73.74   \u001b[0m | \u001b[0m 75.31   \u001b[0m | \u001b[0m 0.3222  \u001b[0m | \u001b[0m 6.282   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.541   \u001b[0m | \u001b[0m 2.872   \u001b[0m | \u001b[0m 29.45   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 0.06043 \u001b[0m | \u001b[0m 6.801   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.4846  \u001b[0m | \u001b[0m 4.17    \u001b[0m | \u001b[0m 37.09   \u001b[0m | \u001b[0m 48.14   \u001b[0m | \u001b[0m 0.4997  \u001b[0m | \u001b[0m 0.6926  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.5504  \u001b[0m | \u001b[0m 2.136   \u001b[0m | \u001b[0m 30.75   \u001b[0m | \u001b[0m 73.84   \u001b[0m | \u001b[0m 0.1781  \u001b[0m | \u001b[0m 0.6172  \u001b[0m |\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m 0.5622  \u001b[0m | \u001b[95m 7.524   \u001b[0m | \u001b[95m 29.53   \u001b[0m | \u001b[95m 8.327   \u001b[0m | \u001b[95m 0.099   \u001b[0m | \u001b[95m 6.809   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 1.918   \u001b[0m | \u001b[0m 60.04   \u001b[0m | \u001b[0m 84.89   \u001b[0m | \u001b[0m 0.2352  \u001b[0m | \u001b[0m 5.513   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 1.734   \u001b[0m | \u001b[0m 36.45   \u001b[0m | \u001b[0m 90.65   \u001b[0m | \u001b[0m 0.3178  \u001b[0m | \u001b[0m 2.425   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.5502  \u001b[0m | \u001b[0m 4.153   \u001b[0m | \u001b[0m 68.47   \u001b[0m | \u001b[0m 20.29   \u001b[0m | \u001b[0m 0.3657  \u001b[0m | \u001b[0m 1.119   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.5271  \u001b[0m | \u001b[0m 3.934   \u001b[0m | \u001b[0m 46.59   \u001b[0m | \u001b[0m 40.22   \u001b[0m | \u001b[0m 0.2809  \u001b[0m | \u001b[0m 1.295   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.4874  \u001b[0m | \u001b[0m 2.772   \u001b[0m | \u001b[0m 27.34   \u001b[0m | \u001b[0m 78.75   \u001b[0m | \u001b[0m 0.3364  \u001b[0m | \u001b[0m 2.796   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.5456  \u001b[0m | \u001b[0m 2.339   \u001b[0m | \u001b[0m 70.19   \u001b[0m | \u001b[0m 86.32   \u001b[0m | \u001b[0m 0.2372  \u001b[0m | \u001b[0m 2.086   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.5577  \u001b[0m | \u001b[0m 1.403   \u001b[0m | \u001b[0m 57.62   \u001b[0m | \u001b[0m 28.2    \u001b[0m | \u001b[0m 0.1839  \u001b[0m | \u001b[0m 3.389   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.5317  \u001b[0m | \u001b[0m 8.067   \u001b[0m | \u001b[0m 35.12   \u001b[0m | \u001b[0m 16.02   \u001b[0m | \u001b[0m 0.1429  \u001b[0m | \u001b[0m 1.028   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.5105  \u001b[0m | \u001b[0m 5.537   \u001b[0m | \u001b[0m 96.07   \u001b[0m | \u001b[0m 99.04   \u001b[0m | \u001b[0m 0.1779  \u001b[0m | \u001b[0m 5.295   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.555   \u001b[0m | \u001b[0m 2.181   \u001b[0m | \u001b[0m 71.83   \u001b[0m | \u001b[0m 28.65   \u001b[0m | \u001b[0m 0.2402  \u001b[0m | \u001b[0m 4.317   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.5362  \u001b[0m | \u001b[0m 8.462   \u001b[0m | \u001b[0m 27.36   \u001b[0m | \u001b[0m 9.879   \u001b[0m | \u001b[0m 0.3768  \u001b[0m | \u001b[0m 6.755   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.5106  \u001b[0m | \u001b[0m 7.923   \u001b[0m | \u001b[0m 30.82   \u001b[0m | \u001b[0m 6.567   \u001b[0m | \u001b[0m 0.3925  \u001b[0m | \u001b[0m 5.446   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.548   \u001b[0m | \u001b[0m 0.0739  \u001b[0m | \u001b[0m 87.77   \u001b[0m | \u001b[0m 25.88   \u001b[0m | \u001b[0m 0.3461  \u001b[0m | \u001b[0m 2.83    \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.4916  \u001b[0m | \u001b[0m 8.16    \u001b[0m | \u001b[0m 23.88   \u001b[0m | \u001b[0m 35.7    \u001b[0m | \u001b[0m 0.4866  \u001b[0m | \u001b[0m 4.193   \u001b[0m |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run Bayesian Optimization\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa8f51d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': <keras.layers.advanced_activations.LeakyReLU at 0x25cc84880a0>,\n",
       " 'batch_size': 29.52797964396342,\n",
       " 'epochs': 8.326870064552317,\n",
       " 'learning_rate': 0.09900103326155506,\n",
       " 'optimizer': 6.80891781550587}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract max parameters\n",
    "params_nn_ = nn_bo.max['params']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a2465",
   "metadata": {},
   "source": [
    "## Sex data with hypertuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc6079a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 55.18% (9.12%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.64      0.58        53\n",
      "           1       0.55      0.43      0.48        54\n",
      "\n",
      "    accuracy                           0.53       107\n",
      "   macro avg       0.54      0.53      0.53       107\n",
      "weighted avg       0.54      0.53      0.53       107\n",
      "\n",
      "Model F1 Score  0.5327102803738317\n"
     ]
    }
   ],
   "source": [
    "#run model with tuned parameters\n",
    "def baseline_model():\n",
    "    opt = Adam(lr = 0.099)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_dim=7, activation=LeakyReLU))\n",
    "    model.add(Dense(7, activation=LeakyReLU))\n",
    "    #model.add(Dropout(0.05, seed=123))\n",
    "    model.add(Dense(7, activation=LeakyReLU))\n",
    "    model.add(Dense(4, activation=LeakyReLU))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=8, batch_size=30, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=20, shuffle=True)\n",
    "results = cross_val_score(estimator, X_sex, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "estimator.fit(X_sex, y_train, epochs=8, batch_size=30, verbose=0)\n",
    "pred = estimator.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Model F1 Score \", f1_score(y_test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2ff3f",
   "metadata": {},
   "source": [
    "## train test accuracy chart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59faa25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create train test accuracy chart for age\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "#train test accuracy for each age model\n",
    "train = [35,30,29]\n",
    "test = [39,35,33]\n",
    "#labels for \n",
    "labels = ['Logistic Regression','XGBoost','Neural Network']\n",
    "\n",
    "#create bar chart\n",
    "x = np.arange(len(labels))\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, train, width, label='Train')\n",
    "rects2 = ax.bar(x + width/2, test, width, label='Test')\n",
    "ax.set_ylim([0, 70])\n",
    "\n",
    "ax.set_ylabel('Accuracy Scores')\n",
    "ax.set_xticks(x, labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "plt.axhline(y = 30.8, color = 'r', linestyle = '-')\n",
    "\n",
    "\n",
    "#plt.title('Age: Train/Test Accuracy per Classification Model')\n",
    "#ax.set_xlabel('Classification Model')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('C:\\\\Users\\\\njeri\\\\Documents\\\\Masters\\\\Thesis\\\\Draft\\\\Images\\\\age accuracy ppt.png');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab55cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create train test accuracy chart for sex\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "#train test accuracy for each sex model\n",
    "train = [55,49,55]\n",
    "test = [54,51,53]\n",
    "labels = ['Logistic Regression','XGBoost','Neural Network']\n",
    "\n",
    "#create bar chart\n",
    "x = np.arange(len(labels))\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0, 70])\n",
    "rects1 = ax.bar(x - width/2, train, width, label='Train')\n",
    "rects2 = ax.bar(x + width/2, test, width, label='Test')\n",
    "\n",
    "ax.set_ylabel('Accuracy Scores')\n",
    "#plt.title('Gender: Train/Test Accuracy per Classification Model')\n",
    "#ax.set_xlabel('Classification Model')\n",
    "ax.set_xticks(x, labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "plt.axhline(y = 50.4, color = 'r', linestyle = '-')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('C:\\\\Users\\\\njeri\\\\Documents\\\\Masters\\\\Thesis\\\\Draft\\\\Images\\\\sex accuracy ppt.png')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
